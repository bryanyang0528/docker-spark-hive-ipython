{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SparkSQL Lab: **\n",
    "#### From this lab, you would write code to execute SQL query in Spark. Makes your analytic life simpler and faster.\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1:* Linking with SparkSQL\n",
    "#### *Part 2:* Loading data programmatically\n",
    "#### *Part 3:* User-Defined Functions \n",
    "#### *Part 4:* Caching for performance \n",
    "#### *Part 5:* Your show time - How many authors tagged as spam?\n",
    "#### Reference for Spark RDD [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 1: Linking with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext, Row\n",
    "sqlContext= HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HiveContext, a superset of SQLContext, was recommended for most use cases. Please make sure you are using HiveContext now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: Loading data programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Read local JSON file to DataFrame **\n",
    "#### Now, try to read json file from Spark Example. Thank for the hashed spam data from PIXNET [PIXNET HACKATHON 2015](https://pixnethackathon2015.events.pixnet.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfile = \"file:///opt/spark-1.4.1-bin-hadoop2.6/examples/src/main/resources/people.json\"\n",
    "df = sqlContext.read.load(jsonfile, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Show time: Query top 2 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "df.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print df's schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Read from Hive **\n",
    "####Don't forget the configuration of Hive should be done by placing your hive-site.xml file in conf/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this class, you will use *pixnet_user_log_1000* for further works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the schema! What do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT * FROM pixnet_user_log_1000\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many rows in *pixnet_user_log*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "df2 = sqlContext.sql(\"SELECT * FROM pixnet_user_log_1000\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print df2.count()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.select('time').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 3: User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###In part 3, you will create your first UDF in Spark SQL with elegant lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#registers this RDD as a temporary table using the given name.\n",
    "df2.registerTempTable(\"people\")\n",
    "\n",
    "# Create an UDF for how long some text is\n",
    "# example from user guide, length function\n",
    "sqlContext.registerFunction(\"strLenPython\", lambda x: len(x)) \n",
    "\n",
    "# split function for parser\n",
    "sqlContext.registerFunction(\"strDate\", lambda x: x.split(\"T\")[0])\n",
    "\n",
    "# put udf with expected columns\n",
    "results = sqlContext.sql(\"SELECT author, \\\n",
    "                                 strDate(time) AS dt, \\\n",
    "                                 strLenPython(action) AS lenAct \\\n",
    "                            FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print top 5 results\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 4: Caching for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Saving to persistent tables**\n",
    "####  `saveAsTable ` : Saves the contents of this DataFrame to a data source as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "sqlContext.sql(\"SELECT * FROM people\").count()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT   strDate(time) AS dt,\\\n",
    "                         count(distinct author) AS cnt \\\n",
    "                FROM     people \\\n",
    "                GROUP BY strDate(time)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.uncacheTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 5: Your show time - How many authors are tagged as spam from pixnet_user_log?\n",
    "\n",
    "#### Here are two hive tables you will need:\n",
    "#### (1) author with action *pixnet_user_spam*\n",
    "#### (2) author with spam tag *pixnet_user_log_1000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "result = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to stop sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
